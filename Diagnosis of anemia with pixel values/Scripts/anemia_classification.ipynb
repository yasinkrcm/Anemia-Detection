{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Machine Learning Pipeline\n",
    "\n",
    "This notebook performs data processing, applies SMOTE for balancing, and evaluates various classification models.\n",
    "\n",
    "The following steps are included:\n",
    "1. Data loading and preprocessing\n",
    "2. Feature engineering and scaling\n",
    "3. Model training and evaluation\n",
    "4. Cross-validation for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\yasin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\yasin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yasin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yasin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yasin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\yasin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\yasin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yasin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yasin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yasin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "In this section, we load the data and perform preprocessing including encoding categorical features, scaling numeric features, and creating additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('..//Datas/output.csv')\n",
    "\n",
    "# Convert 'Gender' column\n",
    "data['Gender'] = data['Gender'].map({'F': 0, 'M': 1, 'F ': 0, 'M ': 1})\n",
    "\n",
    "# Encode the 'Anaemic' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['Anaemic'] = label_encoder.fit_transform(data['Anaemic'])\n",
    "\n",
    "# Apply one-hot encoding to 'Anaemic'\n",
    "one_hot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "anaemic_encoded = one_hot_encoder.fit_transform(data[['Anaemic']])\n",
    "anaemic_encoded_df = pd.DataFrame(anaemic_encoded, columns=one_hot_encoder.get_feature_names_out(['Anaemic']))\n",
    "\n",
    "# Drop the old 'Anaemic' column and add new one-hot encoded columns\n",
    "data = pd.concat([data.drop(columns=['Anaemic']), anaemic_encoded_df], axis=1)\n",
    "\n",
    "# Apply MinMaxScaler to specific columns\n",
    "min_max_scaler = MinMaxScaler()\n",
    "columns_to_scale = ['%Red Pixel', '%Green pixel', '%Blue pixel', 'Hb']\n",
    "data[columns_to_scale] = min_max_scaler.fit_transform(data[columns_to_scale])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "In this section, additional features are created such as average pixel values, differences, and logarithmic transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average pixel, differences, and logarithms\n",
    "data['Average Pixel'] = data[['%Red Pixel', '%Green pixel', '%Blue pixel']].mean(axis=1)\n",
    "data['Red-Blue Difference'] = data['%Red Pixel'] - data['%Blue pixel']\n",
    "data['Green-Red Difference'] = data['%Green pixel'] - data['%Red Pixel']\n",
    "data['Red-Blue Difference'] = data['Red-Blue Difference'].abs()\n",
    "data['Green-Red Difference'] = data['Green-Red Difference'].abs()\n",
    "\n",
    "# Apply MinMaxScaler to the difference columns\n",
    "data[['Red-Blue Difference', 'Green-Red Difference']] = min_max_scaler.fit_transform(data[['Red-Blue Difference', 'Green-Red Difference']])\n",
    "\n",
    "# Calculate log of Hb\n",
    "data['Log Hb'] = np.log(data['Hb'] + 1)\n",
    "\n",
    "# Replace zero values in 'Log Hb' with the minimum positive value\n",
    "min_positive_value = data[data['Log Hb'] > 0]['Log Hb'].min()\n",
    "data['Log Hb'] = data['Log Hb'].replace(0, min_positive_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Columns and Risk Calculation\n",
    "\n",
    "This section includes additional calculations for new columns and risk assessment based on predefined criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional columns and risk calculations\n",
    "data['Red-Blue Product'] = data['%Red Pixel'] * data['%Blue pixel']\n",
    "data['Hb to Red Ratio'] = data['Hb'] / (data['%Red Pixel'] + 1)\n",
    "\n",
    "# Load original dataset and calculate risk\n",
    "original_data = pd.read_csv('..//Datas/output.csv')\n",
    "original_data['Risk'] = np.where((original_data['Hb'] < 12) & (original_data['%Red Pixel'] > 0.7), 1, 0)\n",
    "\n",
    "# Map the risk values to the processed data\n",
    "original_data.set_index('Number', inplace=True)\n",
    "risk_mapping = original_data['Risk'].to_dict()\n",
    "data['Risk'] = data['Number'].map(risk_mapping)\n",
    "\n",
    "# Adjust risk values, setting normal risk to 0.5\n",
    "data['Risk'] = np.where(data['Risk'] == 0, 0.5, data['Risk'])\n",
    "balanced_data = data\n",
    "balanced_data.to_csv('..//Datas/Balanced_Processed_Data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "In this section, various classification models are trained and evaluated on the processed data. The performance of each model is assessed using accuracy, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Results for Logistic Regression:\n",
      "Accuracy: 0.9375\n",
      "Confusion Matrix:\n",
      " [[24  1]\n",
      " [ 1  6]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96        25\n",
      "         1.0       0.86      0.86      0.86         7\n",
      "\n",
      "    accuracy                           0.94        32\n",
      "   macro avg       0.91      0.91      0.91        32\n",
      "weighted avg       0.94      0.94      0.94        32\n",
      "\n",
      "==================================================\n",
      "Training Random Forest...\n",
      "Results for Random Forest:\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[25  0]\n",
      " [ 0  7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        25\n",
      "         1.0       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        32\n",
      "   macro avg       1.00      1.00      1.00        32\n",
      "weighted avg       1.00      1.00      1.00        32\n",
      "\n",
      "==================================================\n",
      "Training Support Vector Machine...\n",
      "Results for Support Vector Machine:\n",
      "Accuracy: 0.90625\n",
      "Confusion Matrix:\n",
      " [[24  1]\n",
      " [ 2  5]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94        25\n",
      "         1.0       0.83      0.71      0.77         7\n",
      "\n",
      "    accuracy                           0.91        32\n",
      "   macro avg       0.88      0.84      0.86        32\n",
      "weighted avg       0.90      0.91      0.90        32\n",
      "\n",
      "==================================================\n",
      "Training K-Nearest Neighbors...\n",
      "Results for K-Nearest Neighbors:\n",
      "Accuracy: 0.90625\n",
      "Confusion Matrix:\n",
      " [[24  1]\n",
      " [ 2  5]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94        25\n",
      "         1.0       0.83      0.71      0.77         7\n",
      "\n",
      "    accuracy                           0.91        32\n",
      "   macro avg       0.88      0.84      0.86        32\n",
      "weighted avg       0.90      0.91      0.90        32\n",
      "\n",
      "==================================================\n",
      "Training Decision Tree...\n",
      "Results for Decision Tree:\n",
      "Accuracy: 0.9375\n",
      "Confusion Matrix:\n",
      " [[25  0]\n",
      " [ 2  5]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96        25\n",
      "         1.0       1.00      0.71      0.83         7\n",
      "\n",
      "    accuracy                           0.94        32\n",
      "   macro avg       0.96      0.86      0.90        32\n",
      "weighted avg       0.94      0.94      0.93        32\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the updated dataset\n",
    "data = pd.read_csv('..//Datas/Balanced_Processed_Data.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['Anaemic_1'])\n",
    "y = data['Anaemic_1']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define and train models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f'Training {model_name}...')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f'Results for {model_name}:')\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "    print('='*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation for Random Forest\n",
    "\n",
    "This section performs cross-validation on the Random Forest model to evaluate its performance more robustly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Accuracy Scores:  [0.95238095 1.         1.         1.         0.9       ]\n",
      "Average Accuracy:  0.9704761904761906\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Forest with cross-validation\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "cv_scores = cross_val_score(rf_model, X, y, cv=5)\n",
    "\n",
    "# Print cross-validation results\n",
    "print('Cross-validation Accuracy Scores: ', cv_scores)\n",
    "print('Average Accuracy: ', cv_scores.mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
